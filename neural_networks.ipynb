{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze mit Python und TensorFlow\n",
    "\n",
    "Der vorherige Beitrag *Link* ist eine Einführung in TensorFlow und eine Sammlung von relevante Links. Da findet man auch eine Erklärung wie man TensorFlow installieren kann. \n",
    "\n",
    "In diesem Blogbeitrag werden wir einen Überblick über Neuronale Netze bekommen und wir werden eins implementieren. Es ist empfohlen, dass man sich den vorherigen Blogbeitrag anschaut, um besseren Überblick über TensorFlow zu bekommen.\n",
    "\n",
    "## Echte Neuronale Netzwerke\n",
    "\n",
    "*Bild von biologischen Neuron*\n",
    "\n",
    "Das ist ein Neuron. Im menschlichen Gehird gibt es 86 Milliarden davon. Die Dendriten bilden Kontaktstellen für andere Zellen, deren Erregung hier auf die Nervenzelle übertragen werden kann. \n",
    "\n",
    "**TODO: Continue Description**\n",
    "\n",
    "## Künstliche Neuronale Netz\n",
    "\n",
    "Perzeptronen und Sigmoidneurone sind die Hauptbestandteile eines neuronalen Netzes. \n",
    "\n",
    "*Bild von einem Perzeptron*\n",
    "\n",
    "Ähnlich wie ein Neuron, haben Perzeptronen mehrere **Inputs** (*x*) mit entsprechende **Gewichtungen** (*w*). Die Inputs sind immer 0 oder 1. Jedes Input wird mit einer der Gewichtungen multipliziert und man addiert noch ein **Bias** (*b*) dazu. Am Ende summiert man alle Ergebnisse und verwendet einen **Schwellwert** um zu entscheiden, ob 1 oder 0 ausgegeben wird.\n",
    "\n",
    "**TODO: 'Bias' in German**\n",
    "\n",
    "Perzeptronen sind oft zu primitiv für künstliche neuronale Netze, deswegen benutzt man Sigmoidneuronen. Die Unterschied ist, dass sie auch eine **Aktivierungsfunktion** haben. \n",
    "\n",
    "*Bild von Sigmoidfunktion*\n",
    "\n",
    "Diese Funktion erlaubt, dass diese Sigmoidneuronen als Inputs Zahlen zwischen 0 und 1 bekommen. Das hilft so, dass wenn man kleine Anpassungen an den Gewichtungen und Biasen macht, dann gibt es auch kleine Unterschiede bei der Ausgabe.\n",
    "\n",
    "Ein neuronales Netz besteht aus mehrere Schichten und jede Schicht hat mehrere Neuronen, wobei in der Regel hat jedes Neuron aus einer Schicht eine Verbindung zu allen anderen Neuronen aus der nächste Schicht. Die erste Schicht ist die Inputschicht, wo man sein Datensatz eingibt. Es kann beliebig viele Zwischenschichten geben, aber die letzte ist die Outputschicht, mit 1 oder mehreren Neuronen (Die Anzahl hängt vom Problem, den man lösen möchte ab). \n",
    "\n",
    "## Implementierung\n",
    "\n",
    "Wir werden ein eifaches neuronales Netz mit TensorFlow implementieren. Zusätzlich brauchen wir aber einige Funktionen aus *scikit-learn*. Wir werden den [Iris-Datensatz](https://en.wikipedia.org/wiki/Iris_flower_data_set) benutzen und versuchen mit dem neuronalen Netz die Blumen richtig zu klassifizieren.\n",
    "\n",
    "Zum ersten wollen wir Tensorflow und einige Funktionen aus *scikit-learn* importieren. Dann laden wir den Datensatz.\n",
    "\n",
    "**TODO: Upload pictures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datensatz\n",
    "data = load_iris()\n",
    "features = data.data\n",
    "labels = data.target.reshape((-1, 1))\n",
    "\n",
    "# Klassen von Blumen zeigen\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man hier sieht, sind die Blummenklassen mit 0, 1 oder 2 bezeichnet. In der Regel, wenn man neuronale Netze für Klassifizierungsprobleme benutzen will, hat man so viele Neuronen in der letzten Schicht wie Klassen. Wir erwarten für jede Stichprobe eine Ausgabe, die so Aussieht `[0.15, 0.70, 0.15]`. In diesem Beispiel ist die Klasse der Blume `1`, weil die zweite Zahl die größte ist.  \n",
    "\n",
    "Wir wollen aber die Vorhersagen des neuronalen Netzes mit den echten Klassen vergleichen, deswegen wandeln wir die numerische Darstellung der Klassen zur sogenannten *One-hot encoding*. So hat man eine 3-Array als Bezeichner für jede Klasse.\n",
    "\n",
    "Beispiel:\n",
    "```\n",
    "Klasse 0 -> [1, 0, 0]\n",
    "Klasse 1 -> [0, 1, 0]\n",
    "Klasse 2 -> [0, 0, 1]\n",
    "```\n",
    "\n",
    "Wir verwenden dafür die `OneHotEncoder` von *scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Modell gut validieren zu können, sollen wir Daten verwenden, mit denen nie trainiert worden ist. Deswegen benutzen wir die `train_test_split` Funktion, um 20 Prozent der Daten als Testdaten zu nehmen. Dann merken wir uns die Anzahl von Merkmale bzw. von Klassen. In diesem Fall ist `x_size = 4` und `y_size = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trainings- und Testdaten erzeugen\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "                                features, \n",
    "                                enc.transform(labels) )\n",
    "\n",
    "x_size = train_x.shape[1]\n",
    "y_size = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, x_size])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, y_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "def hidden_layer(t_input, w_shape, activation=tf.nn.sigmoid):\n",
    "    biases = tf.Variable(tf.random_normal(w_shape))\n",
    "    weights = tf.Variable(tf.random_normal(w_shape))\n",
    "    return activation(tf.add(biases, tf.matmul(t_input, weights)))\n",
    "\n",
    "h_layer1 = hidden_layer(X, [x_size, 128])\n",
    "h_layer2 = hidden_layer(h_layer1, [128, 128])\n",
    "y_hat = hidden_layer(h_layer2, [128, y_size], tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_layer1 = tf.layers.dense(X, 128, activation=tf.nn.sigmoid)\n",
    "h_layer2 = tf.layers.dense(h_layer1, 128, activation=tf.nn.sigmoid)\n",
    "y_hat = tf.layers.dense(h_layer2, y_size, activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss and Optimization\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=Y, logits=y_hat))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.005).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(300):\n",
    "    for i in range(train_x.shape[0]):\n",
    "        sess.run(train_step, feed_dict={X: train_x[i:i+1], Y: train_y[i: i+1]})                                                 \n",
    "        \n",
    "    if (epoch % 10 == 0):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(Y, 1))                                                      \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))                                                       \n",
    "        print(\"Epoch %d: Accuraccy = %f, Loss = %f\" % (                                                                          \n",
    "                          epoch,                                                                                                   \n",
    "                          sess.run(accuracy, feed_dict={X: test_x, Y: test_y}),                                                    \n",
    "                          sess.run(loss, feed_dict={X: train_x, Y: train_y})))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
